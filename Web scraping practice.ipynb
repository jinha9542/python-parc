{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naver news web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# NAVER_SEARCH_URL = \"https://search.naver.com/search.naver\"\n",
    "# query = \"무역전쟁\"\n",
    "# news_list = []\n",
    "\n",
    "# #till page 10\n",
    "# for i in range(1,11):\n",
    "#     start_var = 1\n",
    "#     params = {\n",
    "#         'where': 'news',\n",
    "#         'query': query,\n",
    "#         'start' : (i*10)+1,\n",
    "#         'sm': 'tab_jum',\n",
    "#         'ie': 'utf8',\n",
    "#     }\n",
    "\n",
    "#     resp = requests.get(NAVER_SEARCH_URL, params)\n",
    "#     # 검색 결과 soup\n",
    "#     soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "\n",
    "#     ul_contents = soup.find('ul', class_ = 'type01')\n",
    "    \n",
    "#     new_ul_contents = []\n",
    "#     for contents in ul_contents.contents:\n",
    "#         if not str(contents).strip():\n",
    "#             continue\n",
    "#         new_ul_contents.append(contents)\n",
    "    \n",
    "    \n",
    "\n",
    "#     for li in new_ul_contents:\n",
    "        \n",
    "#         news_dict = {}\n",
    "#         a_tag = li.find('dl').find('dt').find('a')\n",
    "\n",
    "#         news_dict['link'] = a_tag['href']\n",
    "#         news_dict['title'] = a_tag.text\n",
    "        \n",
    "#         print(news_dict['link'], \"entering\")\n",
    "#         detail_resp = requests.get(news_dict['link'])\n",
    "#         detail_soup = BeautifulSoup(detail_resp.content,'html.parser')\n",
    "#         news_dict['body'] = detail_soup.find('body')\n",
    "    \n",
    "#         news_list.append(news_dict)\n",
    "    \n",
    "#     print(news_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실행순서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. url을 파악한다.\n",
    "2. url에 접속한다.\n",
    "3. html 파일을 soup으로 만든다.\n",
    "4. soup에서 각 항목들을 정리한다\n",
    "5. soup에서 상세페이지의 href를 가져온다.\n",
    "6. 각 상세페이지의 href를 requests.get()한다.\n",
    "7. soup로 만들고 상세페이지를 각 항목들을 정리한다.\n",
    "8. 반복문을 통해서 pagination을 이동한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rank': '1', 'title': '2019 6월 모의고사'}, {'rank': '2', 'title': '1분기 성장률 - 0.4%'}, {'rank': '3', 'title': '홍카레오 유시민'}, {'rank': '4', 'title': '60대 한국인 남성으로 확인'}, {'rank': '5', 'title': '김정은 집단체조 관람 수행'}, {'rank': '6', 'title': '5월 소비자물가 0.7%↑'}, {'rank': '7', 'title': '시신 2구 수습'}, {'rank': '8', 'title': '조재범 전 코치'}, {'rank': '9', 'title': '다뉴브강에 울려퍼진 아리랑'}, {'rank': '10', 'title': '검찰은 사형 구형'}]\n",
      "[{'rank': '1', 'title': '퍼퓸 고원희'}, {'rank': '2', 'title': '동상이몽2 박효주'}, {'rank': '3', 'title': '검법남녀2 첫방'}, {'rank': '4', 'title': '박찬호 이어 두번째'}, {'rank': '5', 'title': '바람이 분다 김하늘'}, {'rank': '6', 'title': '어비스 권수현'}, {'rank': '7', 'title': '라디오스타 라이머'}, {'rank': '8', 'title': '너무 행복해서 고민'}, {'rank': '9', 'title': '만나는 사람 있다'}, {'rank': '10', 'title': '냉부해 여에스더'}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "NAVER_SEARCH_URL = \"https://search.naver.com/search.naver\"\n",
    "query = \"무역전쟁\"\n",
    "params = {'where': 'news','query': query, 'sm': 'tab_jum','ie': 'utf8'}\n",
    "resp = requests.get(NAVER_SEARCH_URL, params)\n",
    "soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "\n",
    "ul_contents = soup.find(class_ = \"realtime_srch _aside_news_tab\").find('ol')\n",
    "\n",
    "new_list = []\n",
    "for ol in ul_contents:\n",
    "    \n",
    "    rank_dict = {}\n",
    "    rank_dict['rank'] = ol.span.em.text\n",
    "    rank_dict['title'] = ol.span.span.text\n",
    "    new_list.append(rank_dict)\n",
    "print(new_list)\n",
    "\n",
    "list2 = []\n",
    "\n",
    "ul2_contents = soup.find(class_ = \"realtime_srch _aside_news_tab\").find('ol').findNext('ol')\n",
    "\n",
    "for ol2 in ul2_contents:\n",
    "    \n",
    "    rank_dict = {}\n",
    "    \n",
    "    rank_dict['rank'] = ol2.span.em.text\n",
    "    rank_dict['title'] = ol2.span.span.text\n",
    "    list2.append(rank_dict)\n",
    "print(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rank': '1', 'title': '2019 6월 모의고사'},\n",
      " {'rank': '2', 'title': '1분기 성장률 - 0.4%'},\n",
      " {'rank': '3', 'title': '홍카레오 유시민'},\n",
      " {'rank': '4', 'title': '60대 한국인 남성으로 확인'},\n",
      " {'rank': '5', 'title': '김정은 집단체조 관람 수행'},\n",
      " {'rank': '6', 'title': '5월 소비자물가 0.7%↑'},\n",
      " {'rank': '7', 'title': '시신 2구 수습'},\n",
      " {'rank': '8', 'title': '조재범 전 코치'},\n",
      " {'rank': '9', 'title': '다뉴브강에 울려퍼진 아리랑'},\n",
      " {'rank': '10', 'title': '검찰은 사형 구형'}]\n",
      "--------------------------------------------------\n",
      "[{'rank': '1', 'title': '퍼퓸 고원희'},\n",
      " {'rank': '2', 'title': '동상이몽2 박효주'},\n",
      " {'rank': '3', 'title': '검법남녀2 첫방'},\n",
      " {'rank': '4', 'title': '박찬호 이어 두번째'},\n",
      " {'rank': '5', 'title': '바람이 분다 김하늘'},\n",
      " {'rank': '6', 'title': '어비스 권수현'},\n",
      " {'rank': '7', 'title': '라디오스타 라이머'},\n",
      " {'rank': '8', 'title': '너무 행복해서 고민'},\n",
      " {'rank': '9', 'title': '만나는 사람 있다'},\n",
      " {'rank': '10', 'title': '냉부해 여에스더'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(new_list)\n",
    "print(\"-\"*50)\n",
    "pprint(list2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'news_topic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e6f76f0ff0e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mli_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnews_topic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnew_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mli_tags\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnew_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnew_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'news_topic' is not defined"
     ]
    }
   ],
   "source": [
    "li_tags = news_topic.contents\n",
    "new_list = []\n",
    "for t in li_tags:\n",
    "    new_list.append(t.text)\n",
    "new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_tag = news_topic.li\n",
    "new_list2 = []\n",
    "new_list2.append(li_tag.text)\n",
    "for x in li_tag.find_next_sibling('li'):\n",
    "    new_list2.append(li.text)\n",
    "new_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_lists = news_topic.find_all('li', recursive = False)\n",
    "new_list3 = []\n",
    "for li in li_list:\n",
    "    title = li.find(\"span\", class_ = \"tit\").text\n",
    "    new_list3.append(title)\n",
    "new_list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(\"https://search.naver.com/search.naver?sm=top_hty&fbm=1&ie=utf8&query=%EB%AC%B4%EC%97%AD%EC%A0%84%EC%9F%81\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_topic_all = soup.find_all('div', id = 'nxfr_htp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(news_topic_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_topic_all[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_topic.find_all('ol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blog scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from selenium import webdriver\n",
    "\n",
    "NAVER_SEARCH_URL = \"https://search.naver.com/search.naver\"\n",
    "query = \"놀란 라이언\"\n",
    "news_list = []\n",
    "\n",
    "#till page 10\n",
    "for i in range(1,11):\n",
    "    start_var = 1\n",
    "    params = {\n",
    "        'where': 'post',\n",
    "        'query': query,\n",
    "        'start' : (i*10)+1,\n",
    "        'sm': 'tab_jum',\n",
    "        'ie': 'utf8',\n",
    "    }\n",
    "\n",
    "    resp = requests.get(NAVER_SEARCH_URL, params)\n",
    "    # 검색 결과 soup\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "\n",
    "    ul_contents = soup.find('ul', class_ = 'type01').dl.dt\n",
    "\n",
    "    for a in ul_contents:\n",
    "        \n",
    "        blog_dict = {}\n",
    "        blog_dict['link'] = a['href']\n",
    "        print (a['title'])\n",
    "        print(blog_dict['link'], \"entering\")\n",
    "        detail_resp = requests.get(blog_dict['link'])\n",
    "        detail_soup = BeautifulSoup(detail_resp.content,'html.parser')\n",
    "        print(detail_soup.body)\n",
    "#         blog_dict['body'] = detail_soup.find('body')\n",
    "    \n",
    "#         news_list.append(news_dict)\n",
    "    \n",
    "#     print(news_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
